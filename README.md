# Llama 3.1 8B NVFP4 QAD Quantization

NVIDIAã®QADï¼ˆQuantization-Aware Distillationï¼‰æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€Llama 3.1 8Bã‚’NVFP4å½¢å¼ã«é‡å­åŒ–ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚99.4%ã®ç²¾åº¦å›å¾©ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **[ç’°å¢ƒæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰](docs/ENVIRONMENT_SETUP.md)** - è©³ç´°ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ï¼ˆWindows/Linux/Dockerï¼‰
- **[ç’°å¢ƒæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](scripts/check_environment.py)** - ç’°å¢ƒã®è‡ªå‹•æ¤œè¨¼

## Overview

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®3æ®µéšã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š

1. **QAD Training** (`src/qad_train.py`): 
   - æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆBF16ï¼‰ã‹ã‚‰çŸ¥è­˜ã‚’è’¸ç•™
   - ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆNVFP4ï¼‰ãŒæ•™å¸«ã®å‡ºåŠ›åˆ†å¸ƒã‚’å­¦ç¿’
   - KL divergenceæå¤± + ã‚¿ã‚¹ã‚¯æå¤±ã®çµ„ã¿åˆã‚ã›

2. **Quantization** (`src/quantize.py`):
   - QADè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’NVFP4å½¢å¼ã«é‡å­åŒ–
   - LLM Compressorã‚’ä½¿ç”¨

3. **Inference** (`src/inference.py`):
   - vLLMã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«–
   - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ä»˜ã

## ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶

### å¿…é ˆè¦ä»¶

- **GPU**: NVIDIA Blackwellã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (SM100+)
  - RTX 5090
  - B100, B200
  - H200 (å°†æ¥ã‚µãƒãƒ¼ãƒˆäºˆå®š)
- **VRAM**: æœ€ä½24GB (RTX 5090)
- **ã‚·ã‚¹ãƒ†ãƒ RAM**: æ¨å¥¨128GB+
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 100GBä»¥ä¸Šã®ç©ºãå®¹é‡

### QADè¨“ç·´æ¨å¥¨æ§‹æˆ

- **8x H100/B200** (80GB each)
- **ã¾ãŸã¯ 8x RTX 5090** (32GB each)
- **æ™‚é–“**: ç´„8-12æ™‚é–“ï¼ˆ50Kã‚µãƒ³ãƒ—ãƒ«ã€3ã‚¨ãƒãƒƒã‚¯ï¼‰

âš ï¸ **æ³¨æ„**: SM100æœªæº€ã®GPUã§ã¯ã€vLLMã¯é‡ã¿ã®ã¿ã®é‡å­åŒ–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³é‡å­åŒ–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Python 3.10+ ãŒå¿…è¦
python --version

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ã¾ãŸã¯
venv\Scripts\activate  # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# ç’°å¢ƒæ¤œè¨¼
python scripts/check_environment.py
```

### 2. ç‰¹åˆ¥ãªè¦ä»¶

#### CUDA 12.4+ ã¨ cuDNN 9.0+

```bash
# CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª
nvcc --version
nvidia-smi
```

#### Flash Attentionï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã ãŒæ¨å¥¨ï¼‰

```bash
# Linuxã®ã¿
pip install flash-attn --no-build-isolation
```

## ä½¿ç”¨æ–¹æ³•

### Phase 1: QADè¨“ç·´

æ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼ˆBF16ï¼‰ã‹ã‚‰çŸ¥è­˜ã‚’è’¸ç•™ã—ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚

```bash
# å˜ä¸€GPUã§ã®è¨“ç·´
python src/qad_train.py \
    --config configs/qad_config.yaml \
    --teacher-model meta-llama/Llama-3.1-8B-Instruct \
    --output-dir ./outputs/llama-3.1-8b-qad \
    --num-epochs 3 \
    --batch-size 1

# ãƒãƒ«ãƒGPUã§ã®è¨“ç·´ï¼ˆæ¨å¥¨ï¼‰
accelerate launch --multi_gpu --num_processes 8 src/qad_train.py \
    --config configs/qad_config.yaml

# DeepSpeedä½¿ç”¨æ™‚
accelerate launch --use_deepspeed src/qad_train.py \
    --config configs/qad_config.yaml
```

### Phase 2: NVFP4é‡å­åŒ–

è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’NVFP4å½¢å¼ã«é‡å­åŒ–ã—ã¾ã™ã€‚

```bash
python src/quantize.py \
    --model-path ./outputs/llama-3.1-8b-qad/checkpoint-final \
    --output-dir ./models/llama-3.1-8b-nvfp4 \
    --num-calibration-samples 512 \
    --max-seq-length 2048
```

### Phase 3: æ¨è«–

#### ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰

```bash
python src/inference.py \
    --model-path ./models/llama-3.1-8b-nvfp4 \
    --interactive \
    --temperature 0.7 \
    --max-tokens 2048
```

#### å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```bash
python src/inference.py \
    --model-path ./models/llama-3.1-8b-nvfp4 \
    --prompt "Explain quantum computing in simple terms."
```

#### ãƒãƒƒãƒå‡¦ç†

```bash
# prompts.txtã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’1è¡Œãšã¤è¨˜è¼‰
python src/inference.py \
    --model-path ./models/llama-3.1-8b-nvfp4 \
    --prompts-file prompts.txt \
    --output results.json
```

#### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```bash
python src/inference.py \
    --model-path ./models/llama-3.1-8b-nvfp4 \
    --benchmark \
    --tensor-parallel-size 1
```

## è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

`configs/qad_config.yaml`ã§è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ï¼š

```yaml
model:
  teacher_model: "meta-llama/Llama-3.1-8B-Instruct"

dataset:
  name: "HuggingFaceH4/ultrachat_200k"
  num_samples: 50000
  max_seq_length: 2048

qad:
  num_epochs: 3
  batch_size: 1
  learning_rate: 1.0e-5
  temperature: 2.0  # è’¸ç•™æ¸©åº¦
  alpha: 0.5        # è’¸ç•™æå¤±ã®é‡ã¿
  
paths:
  output_dir: "./outputs/llama-3.1-8b-nvfp4-qad"
```

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®å®Ÿè¡Œ

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦å…¨è‡ªå‹•å®Ÿè¡Œ
bash scripts/run_full_pipeline.sh \
    meta-llama/Llama-3.1-8B-Instruct \
    ./models/my-llama-nvfp4
```

## æ€§èƒ½æ¯”è¼ƒ

| Metric | BF16 (Baseline) | NVFP4 (QAD) | PTQ Only |
|--------|----------------|-------------|----------|
| Model Size | 16 GB | 4 GB | 4 GB |
| Perplexity | 8.12 | 8.19 | 8.85 |
| Accuracy | 100% | 99.4% | 94.2% |
| Tokens/sec (RTX 5090) | 45 | 180 | 180 |

*æ•°å€¤ã¯ä¾‹ç¤ºçš„ãªã‚‚ã®ã§ã™ã€‚å®Ÿéš›ã®æ€§èƒ½ã¯ç’°å¢ƒã«ã‚ˆã‚Šç•°ãªã‚Šã¾ã™ã€‚

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### OOM (Out of Memory) ã‚¨ãƒ©ãƒ¼

```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
python src/qad_train.py --batch-size 1 --gradient-accumulation-steps 16

# DeepSpeed ZeRO-3ã‚’ä½¿ç”¨
accelerate config  # DeepSpeedè¨­å®š
```

### CUDA Capabilityã‚¨ãƒ©ãƒ¼

SM100æœªæº€ã®GPUã§ã¯ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼š

```
Falling back to weight-only quantization (no activation quantization).
```

ã“ã‚Œã¯æ­£å¸¸ãªå‹•ä½œã§ã™ã€‚NVFP4ã®å®Œå…¨ãªæ€§èƒ½ã‚’å¾—ã‚‹ã«ã¯Blackwell GPUãŒå¿…è¦ã§ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼

HuggingFaceã®èªè¨¼ãŒå¿…è¦ãªå ´åˆï¼š

```bash
huggingface-cli login
# ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
export HF_TOKEN=your_token_here
```

## é‡è¦ãªæ³¨æ„äº‹é …

1. **QADã¯è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹**: PTQï¼ˆPost-Training Quantizationï¼‰ã¨ã¯ç•°ãªã‚Šã€QADã«ã¯ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ãŒå¿…è¦ã§ã™
2. **æ™‚é–“ã¨ãƒªã‚½ãƒ¼ã‚¹**: å®Œå…¨ãªQADè¨“ç·´ã«ã¯æ•°æ™‚é–“ã€œæ•°æ—¥ã‹ã‹ã‚Šã¾ã™
3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ™‚ã®ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã‚ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™
4. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ**: å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã€è¨“ç·´ã®å†é–‹ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„

## å¼•ç”¨

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®ç ”ç©¶ã«åŸºã¥ã„ã¦ã„ã¾ã™ï¼š

```bibtex
@article{quantizationawaredistillation2026nvidia,
  title={Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery},
  author={NVIDIA},
  year={2026}
}
```

## å‚è€ƒãƒªãƒ³ã‚¯

- [NVIDIA QAD Research](https://research.nvidia.com/labs/nemotron/nemotron-qad/)
- [NVIDIA Nemotron-3-Nano-30B-A3B-NVFP4](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4)
- [LLM Compressor Documentation](https://docs.vllm.ai/projects/llm-compressor/)
- [vLLM Documentation](https://docs.vllm.ai/)

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

Llama 3.1ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã¯Metaã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚

## è²¢çŒ®

ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯GitHub Issuesã¸ãŠé¡˜ã„ã—ã¾ã™ã€‚

## ä½œè€…

QADå®Ÿè£…ä½“é¨“ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
